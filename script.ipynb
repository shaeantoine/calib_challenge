{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing \n",
    "\n",
    "In the following code block I've taken the pitch and yaw angles from each of the text files and merged them into a singular dataframe with additional columns for a video and frame index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the text files\n",
    "directory = 'labeled'\n",
    "\n",
    "# List to store the data (video, frame, pitch, yaw)\n",
    "data = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Extract the video title from the filename (e.g., \"video1\")\n",
    "        video_title = filename.split('.')[0]\n",
    "        \n",
    "        # Create the full file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open and read the contents of the text file\n",
    "        with open(filepath, 'r') as file:\n",
    "            content = file.read().strip()\n",
    "            \n",
    "            # Assuming the content contains alternating pitch and yaw values for each frame\n",
    "            values = list(map(float, content.split()))\n",
    "            \n",
    "            # Iterate over the values two at a time (pitch, yaw) and generate frame data\n",
    "            for i in range(0, len(values), 2):\n",
    "                frame_number = i // 2  # Frame index\n",
    "                pitch = values[i]\n",
    "                yaw = values[i + 1]\n",
    "                \n",
    "                # Append a row of data (video title, frame number, pitch, yaw) to the list\n",
    "                data.append([video_title, frame_number, pitch, yaw])\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data, columns=['video', 'frame', 'pitch', 'yaw'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of frames (1200) and labels (1196) do not match for video 4.\n",
      "Total labeled frames: 5996\n",
      "Total labeled frames after removing NaNs: 5019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paths\n",
    "extracted_frames_dir = 'extracted_frames'\n",
    "labeled_dir = 'labeled'\n",
    "unlabeled_dir = 'unlabeled'\n",
    "\n",
    "# Function to load image paths and corresponding labels\n",
    "def load_labeled_data(extracted_frames_dir, labeled_dir):\n",
    "    image_paths = []\n",
    "    pitches = []\n",
    "    yaws = []\n",
    "    \n",
    "    # Iterate over each labeled video\n",
    "    for video_id in range(5):  # Videos 0 to 4\n",
    "        video_folder = f\"video_{video_id}\"\n",
    "        video_frames_dir = os.path.join(extracted_frames_dir, video_folder)\n",
    "        label_file = os.path.join(labeled_dir, f\"{video_id}.txt\")\n",
    "        \n",
    "        if not os.path.exists(label_file):\n",
    "            print(f\"Warning: Label file {label_file} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Load labels from the txt file\n",
    "        labels = np.loadtxt(label_file)\n",
    "        if labels.ndim == 1:\n",
    "            labels = labels.reshape(-1, 2)  # Ensure it's a 2D array\n",
    "        \n",
    "        # Check number of frames and labels match\n",
    "        frame_files = sorted([f for f in os.listdir(video_frames_dir) if f.endswith('.jpg')])\n",
    "        num_frames = len(frame_files)\n",
    "        num_labels = labels.shape[0]\n",
    "        \n",
    "        if num_frames != num_labels:\n",
    "            print(f\"Warning: Number of frames ({num_frames}) and labels ({num_labels}) do not match for video {video_id}.\")\n",
    "            min_length = min(num_frames, num_labels)\n",
    "            frame_files = frame_files[:min_length]\n",
    "            labels = labels[:min_length]\n",
    "        \n",
    "        # Assign labels to each frame\n",
    "        for frame_file, (pitch, yaw) in zip(frame_files, labels):\n",
    "            frame_path = os.path.join(video_frames_dir, frame_file)\n",
    "            image_paths.append(frame_path)\n",
    "            pitches.append(pitch)\n",
    "            yaws.append(yaw)\n",
    "    \n",
    "    return np.array(image_paths), np.array(pitches), np.array(yaws)\n",
    "\n",
    "# Load all labeled data\n",
    "image_paths, pitches, yaws = load_labeled_data(extracted_frames_dir, labeled_dir)\n",
    "\n",
    "print(f\"Total labeled frames: {len(image_paths)}\")\n",
    "\n",
    "# Optionally, remove frames with NaN labels if any (assuming some labels might be NaN)\n",
    "valid_indices = ~np.isnan(pitches) & ~np.isnan(yaws)\n",
    "image_paths = image_paths[valid_indices]\n",
    "pitches = pitches[valid_indices]\n",
    "yaws = yaws[valid_indices]\n",
    "\n",
    "print(f\"Total labeled frames after removing NaNs: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDataGenerator\u001b[39;00m(\u001b[43mSequence\u001b[49m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_paths, pitches, yaws, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, img_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m), augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths \u001b[38;5;241m=\u001b[39m image_paths\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequence' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, pitches, yaws, batch_size=32, img_size=(224, 224), augment=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.pitches = pitches\n",
    "        self.yaws = yaws\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        if self.augment:\n",
    "            self.aug = ImageDataGenerator(\n",
    "                rotation_range=20,\n",
    "                width_shift_range=0.1,\n",
    "                height_shift_range=0.1,\n",
    "                shear_range=0.1,\n",
    "                zoom_range=0.1,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        else:\n",
    "            self.aug = ImageDataGenerator()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.image_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_pitches = self.pitches[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_yaws = self.yaws[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        \n",
    "        images = []\n",
    "        for img_path in batch_x:\n",
    "            img = load_img(img_path, target_size=self.img_size)\n",
    "            img = img_to_array(img)\n",
    "            images.append(img)\n",
    "        \n",
    "        images = np.array(images, dtype='float32') / 255.0  # Normalize\n",
    "        \n",
    "        if self.augment:\n",
    "            # Apply augmentation\n",
    "            images = self.aug.flow(images, batch_size=self.batch_size, shuffle=False).next()\n",
    "        \n",
    "        return images, np.stack([batch_pitches, batch_yaws], axis=1)\n",
    "\n",
    "# Create a single generator for training\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "train_gen = DataGenerator(image_paths, pitches, yaws, batch_size=batch_size, img_size=img_size, augment=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An instance of the data generator \n",
    "\n",
    "import numpy as np\n",
    "from data_generator import DataGenerator\n",
    "\n",
    "# Load processed data\n",
    "image_paths = np.load('scripts/processed_image_paths.npy')\n",
    "pitches = np.load('scripts/processed_pitches.npy')\n",
    "yaws = np.load('scripts/processed_yaws.npy')\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "img_size = (224, 224)\n",
    "\n",
    "# Create Data Generator without augmentation\n",
    "train_gen = DataGenerator(\n",
    "    image_paths=image_paths,\n",
    "    pitches=pitches,\n",
    "    yaws=yaws,\n",
    "    batch_size=batch_size,\n",
    "    img_size=img_size,\n",
    "    augment=False  # No augmentation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition \n",
    "\n",
    "Transfer learning with pre-trained model with options including: \n",
    "- ResNet * (specifically ResNet50)\n",
    "- EfficientNet * (specifically EfficientNetB0)\n",
    "- MobileNet \n",
    "- Inception Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the EffificentNetB0 Model \n",
    "\n",
    "# Define image size and batch size\n",
    "img_size = (224, 224)  # EfficientNetB0 expects 224x224 images\n",
    "batch_size = 32\n",
    "\n",
    "# Load the pre-trained EfficientNetB0 model without the top classification layer\n",
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(*img_size, 3))\n",
    "\n",
    "# Freeze the base model to prevent its weights from being updated during initial training\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Add a fully connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Add another fully connected layer\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Add the final output layer with 2 neurons (pitch and yaw) and linear activation for regression\n",
    "predictions = Dense(2, activation='linear')(x)\n",
    "\n",
    "# Define the complete model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with Mean Squared Error loss and Adam optimizer\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Display the model architecture\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
