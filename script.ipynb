{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma AI Calibration Challenge\n",
    "\n",
    "The purpose of this notebook is to investigate how to translate the video files into usable data. I have 5 HEVC video files each 1 minute long. Each video is filmed at 20 frames per second corresponding to 1200 frames per video. The following code block will read in a HEVC file and produce 1200 frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fontconfig warning: ignoring UTF-8: not a valid region tag\n",
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "import os \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing and Data Augmentation \n",
    "\n",
    "In the following code block I've taken the pitch and yaw angles from each of the text files and merged them into a singular dataframe with additional columns for a video and frame index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the text files\n",
    "directory = 'labeled'\n",
    "\n",
    "# List to store the data (video, frame, pitch, yaw)\n",
    "data = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        # Extract the video title from the filename (e.g., \"video1\")\n",
    "        video_title = filename.split('.')[0]\n",
    "        \n",
    "        # Create the full file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Open and read the contents of the text file\n",
    "        with open(filepath, 'r') as file:\n",
    "            content = file.read().strip()\n",
    "            \n",
    "            # Assuming the content contains alternating pitch and yaw values for each frame\n",
    "            values = list(map(float, content.split()))\n",
    "            \n",
    "            # Iterate over the values two at a time (pitch, yaw) and generate frame data\n",
    "            for i in range(0, len(values), 2):\n",
    "                frame_number = i // 2  # Frame index\n",
    "                pitch = values[i]\n",
    "                yaw = values[i + 1]\n",
    "                \n",
    "                # Append a row of data (video title, frame number, pitch, yaw) to the list\n",
    "                data.append([video_title, frame_number, pitch, yaw])\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data, columns=['video', 'frame', 'pitch', 'yaw'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Number of frames (1200) and labels (1196) do not match for video 4.\n",
      "Total labeled frames: 5996\n",
      "Total labeled frames after removing NaNs: 5019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define paths\n",
    "extracted_frames_dir = 'extracted_frames'\n",
    "labeled_dir = 'labeled'\n",
    "unlabeled_dir = 'unlabeled'\n",
    "\n",
    "# Function to load image paths and corresponding labels\n",
    "def load_labeled_data(extracted_frames_dir, labeled_dir):\n",
    "    image_paths = []\n",
    "    pitches = []\n",
    "    yaws = []\n",
    "    \n",
    "    # Iterate over each labeled video\n",
    "    for video_id in range(5):  # Videos 0 to 4\n",
    "        video_folder = f\"video_{video_id}\"\n",
    "        video_frames_dir = os.path.join(extracted_frames_dir, video_folder)\n",
    "        label_file = os.path.join(labeled_dir, f\"{video_id}.txt\")\n",
    "        \n",
    "        if not os.path.exists(label_file):\n",
    "            print(f\"Warning: Label file {label_file} does not exist.\")\n",
    "            continue\n",
    "        \n",
    "        # Load labels from the txt file\n",
    "        labels = np.loadtxt(label_file)\n",
    "        if labels.ndim == 1:\n",
    "            labels = labels.reshape(-1, 2)  # Ensure it's a 2D array\n",
    "        \n",
    "        # Check number of frames and labels match\n",
    "        frame_files = sorted([f for f in os.listdir(video_frames_dir) if f.endswith('.jpg')])\n",
    "        num_frames = len(frame_files)\n",
    "        num_labels = labels.shape[0]\n",
    "        \n",
    "        if num_frames != num_labels:\n",
    "            print(f\"Warning: Number of frames ({num_frames}) and labels ({num_labels}) do not match for video {video_id}.\")\n",
    "            min_length = min(num_frames, num_labels)\n",
    "            frame_files = frame_files[:min_length]\n",
    "            labels = labels[:min_length]\n",
    "        \n",
    "        # Assign labels to each frame\n",
    "        for frame_file, (pitch, yaw) in zip(frame_files, labels):\n",
    "            frame_path = os.path.join(video_frames_dir, frame_file)\n",
    "            image_paths.append(frame_path)\n",
    "            pitches.append(pitch)\n",
    "            yaws.append(yaw)\n",
    "    \n",
    "    return np.array(image_paths), np.array(pitches), np.array(yaws)\n",
    "\n",
    "# Load all labeled data\n",
    "image_paths, pitches, yaws = load_labeled_data(extracted_frames_dir, labeled_dir)\n",
    "\n",
    "print(f\"Total labeled frames: {len(image_paths)}\")\n",
    "\n",
    "# Optionally, remove frames with NaN labels if any (assuming some labels might be NaN)\n",
    "valid_indices = ~np.isnan(pitches) & ~np.isnan(yaws)\n",
    "image_paths = image_paths[valid_indices]\n",
    "pitches = pitches[valid_indices]\n",
    "yaws = yaws[valid_indices]\n",
    "\n",
    "print(f\"Total labeled frames after removing NaNs: {len(image_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation and Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
