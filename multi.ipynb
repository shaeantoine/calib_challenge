{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Approach\n",
    "\n",
    "The following notebook will attempt to use CNN feature extraction and LSTM for temporal prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process imports\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Feature extraction completed for video extracted_frames/video_9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNNFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract features from video frames using a pretrained CNN (ResNet).\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The pretrained CNN model (ResNet).\n",
    "    transform : torchvision.transforms.Compose\n",
    "        The transformations applied to input frames (resizing, normalization).\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    extract_features(frame_batch: torch.Tensor) -> torch.Tensor:\n",
    "        Extracts features from a batch of frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    #def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    def __init__(self, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the CNN feature extractor with a pretrained ResNet model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        device : str\n",
    "            The device on which to run the model ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        # Remove the classification head (fc layer)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-2])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Define the necessary image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize frame to 224x224 (ResNet input size)\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, frame_batch):\n",
    "        \"\"\"\n",
    "        Extract features from a batch of frames.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame_batch : torch.Tensor\n",
    "            A batch of video frames (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        features : torch.Tensor\n",
    "            Extracted CNN features for each frame in the batch.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            frame_batch = frame_batch.to(self.device)\n",
    "            features = self.model(frame_batch)\n",
    "        return features\n",
    "\n",
    "    def process_frame(self, frame_path):\n",
    "        \"\"\"\n",
    "        Process a single frame from an image file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame_path : str\n",
    "            Path to the image file (frame).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        processed_frame : torch.Tensor\n",
    "            Processed frame ready for feature extraction.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image and apply transformations\n",
    "            frame = Image.open(frame_path).convert(\"RGB\")\n",
    "            processed_frame = self.transform(frame)\n",
    "            return processed_frame\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing frame {frame_path}: {str(e)}\")\n",
    "\n",
    "    def process_batch(self, frame_paths):\n",
    "        \"\"\"\n",
    "        Process a batch of frames from a list of image paths.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame_paths : list of str\n",
    "            List of file paths to the frames.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        frame_batch : torch.Tensor\n",
    "            A batch of processed frames ready for feature extraction.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_frames = [self.process_frame(fp) for fp in frame_paths]\n",
    "            frame_batch = torch.stack(processed_frames)\n",
    "            return frame_batch\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing frame batch: {str(e)}\")\n",
    "\n",
    "    def save_features(self, features, output_dir, video_id):\n",
    "        \"\"\"\n",
    "        Save extracted features to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : torch.Tensor\n",
    "            Extracted features from the CNN.\n",
    "        output_dir : str\n",
    "            Directory to save the features.\n",
    "        video_id : str\n",
    "            Identifier for the video (used in the output filename).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, f\"{video_id}_features.pt\")\n",
    "            torch.save(features.cpu(), output_path)\n",
    "            print(f\"Features saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving features: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def extract_video_features(video_frame_dir, output_dir, extractor):\n",
    "    \"\"\"\n",
    "    Extract features for all frames in a video directory and save to output directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_frame_dir : str\n",
    "        Path to the directory containing video frames.\n",
    "    output_dir : str\n",
    "        Path to the directory where features will be saved.\n",
    "    extractor : CNNFeatureExtractor\n",
    "        The CNN feature extractor instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all frame files in the directory\n",
    "        frame_files = sorted([os.path.join(video_frame_dir, f) for f in os.listdir(video_frame_dir)\n",
    "                              if f.endswith(('.jpg'))])\n",
    "\n",
    "        # Process frames in batches (if needed for larger videos)\n",
    "        batch_size = 16\n",
    "        for i in range(0, len(frame_files), batch_size):\n",
    "            batch_files = frame_files[i:i + batch_size]\n",
    "            frame_batch = extractor.process_batch(batch_files)\n",
    "            features = extractor.extract_features(frame_batch)\n",
    "            extractor.save_features(features, output_dir, video_frame_dir.split('/')[-1])\n",
    "            \n",
    "        print(f\"Feature extraction completed for video {video_frame_dir}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error extracting features for video {video_frame_dir}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Initialize the feature extractor\n",
    "extractor = CNNFeatureExtractor()\n",
    "\n",
    "# Example: Process video frames from a directory and save extracted features\n",
    "video_frame_directory = 'extracted_frames/video_0'\n",
    "#video_frame_directory = 'extracted_frames/video_9'\n",
    "output_directory = 'data'\n",
    "extract_video_features(video_frame_directory, output_directory, extractor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Dataset\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for loading video features and corresponding labels for supervised learning.\n",
    "\n",
    "    This class takes pre-extracted CNN features from dashcam videos, processes them into sequences\n",
    "    suitable for temporal models (like LSTMs), and loads corresponding pitch and yaw angle labels.\n",
    "\n",
    "    Attributes:\n",
    "        data_dir (str): Directory where feature .pt files for each video frame are stored.\n",
    "        labeled_dir (str): Directory where corresponding pitch and yaw angle labels (.txt) are stored.\n",
    "        sequence_length (int): Number of consecutive frames to be grouped into a sequence for input to the model.\n",
    "        transform (callable, optional): Optional transform to be applied on a sequence of features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, labeled_dir, sequence_length=10, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the VideoDataset instance.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing the pre-extracted CNN feature files (.pt).\n",
    "            labeled_dir (str): Path to the directory containing the labeled ground truth files (.txt).\n",
    "            sequence_length (int): Number of frames to group into a sequence for input to the model (default=10).\n",
    "            transform (callable, optional): Optional transform to apply to the feature sequences (default=None).\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir  # Directory for the extracted CNN features.\n",
    "        self.labeled_dir = labeled_dir  # Directory for the ground-truth pitch and yaw angle labels.\n",
    "        self.sequence_length = sequence_length  # Number of consecutive frames used for each sequence.\n",
    "        self.transform = transform  # Optional data transformation (e.g., normalization).\n",
    "        \n",
    "        # Get the list of all video feature files with the '_features.pt' suffix\n",
    "        self.feature_files = sorted([f for f in os.listdir(data_dir) if f.endswith('_features.pt')])\n",
    "        \n",
    "        # Get the list of all label files with the '.txt' suffix (for videos 0 to 4)\n",
    "        self.label_files = sorted([f for f in os.listdir(labeled_dir) if f.endswith('.txt') and int(f.split('.')[0]) < 5])\n",
    "        \n",
    "        # Extract the video index (e.g., '0', '1', etc.) from the filenames of the features and labels\n",
    "        self.indexed_feature_files = [int(f.split('_')[1]) for f in self.feature_files if int(f.split('_')[1]) < 5]\n",
    "        self.indexed_label_files = [int(f.split('.')[0]) for f in self.label_files]\n",
    "        \n",
    "        # Create a mapping between feature files and label files based on the video index\n",
    "        self.feature_label_map = {f: f\"{index}.txt\" for f, index in zip(self.feature_files, self.indexed_feature_files)}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of sequences in the dataset. This is the total number of frames divided by the sequence length.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of sequences in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.feature_files) // self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sequence of features and the corresponding label.\n",
    "\n",
    "        For each sequence, consecutive frames are loaded and stacked together to form a 3D tensor \n",
    "        (sequence_length, feature_dim). The corresponding label is retrieved from the last frame of the sequence.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index for the sequence.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the sequence of features (torch.Tensor) and the corresponding label (torch.Tensor).\n",
    "        \"\"\"\n",
    "        # Calculate the start and end indices for the sequence of frames\n",
    "        start_idx = idx * self.sequence_length\n",
    "        end_idx = start_idx + self.sequence_length\n",
    "        \n",
    "        # Initialize a list to store feature tensors for the sequence\n",
    "        sequence_features = []\n",
    "        \n",
    "        # Load and append features from each frame in the sequence\n",
    "        for i in range(start_idx, end_idx):\n",
    "            feature_file = self.feature_files[i]  # Get the filename for the current frame's features\n",
    "            feature_path = os.path.join(self.data_dir, feature_file)  # Full path to the .pt file\n",
    "            features = torch.load(feature_path)  # Load the feature tensor\n",
    "            sequence_features.append(features)  # Append it to the sequence list\n",
    "        \n",
    "        # Stack the list of feature tensors into a single tensor of shape (sequence_length, feature_dim)\n",
    "        sequence_features = torch.stack(sequence_features)\n",
    "        \n",
    "        # Load the corresponding label file for the last frame in the sequence\n",
    "        feature_file = self.feature_files[end_idx - 1]  # Get the filename for the last frame in the sequence\n",
    "        video_idx = int(feature_file.split('_')[1])  # Extract the video index (e.g., '0', '1', etc.)\n",
    "        \n",
    "        # Ensure that we only get labels for valid videos (0 to 4)\n",
    "        if video_idx in self.indexed_label_files:\n",
    "            label_file = self.feature_label_map[feature_file]  # Map the feature file to the label file\n",
    "            label_path = os.path.join(self.labeled_dir, label_file)  # Full path to the .txt label file\n",
    "            \n",
    "            # Read the label from the .txt file (expecting a single line with pitch and yaw values)\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = list(map(float, f.readline().split()))  # Convert the label string to a list of floats (pitch, yaw)\n",
    "            \n",
    "            # If a transformation is provided (e.g., normalization), apply it to the sequence of features\n",
    "            if self.transform:\n",
    "                sequence_features = self.transform(sequence_features)\n",
    "            \n",
    "            # Return the sequence of features and the corresponding label as tensors\n",
    "            return sequence_features, torch.tensor(label)\n",
    "        \n",
    "        # If no label exists for the video index (e.g., videos 5-9), raise an exception\n",
    "        else:\n",
    "            return sequence_features, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based Regressor for predicting pitch and yaw angles from CNN-extracted features.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        Size of the input feature vector per frame.\n",
    "    hidden_size : int\n",
    "        Number of features in the hidden state of the LSTM.\n",
    "    num_layers : int\n",
    "        Number of recurrent layers in the LSTM.\n",
    "    dropout : float\n",
    "        Dropout probability between LSTM layers.\n",
    "    bidirectional : bool\n",
    "        If True, becomes a bidirectional LSTM.\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x):\n",
    "        Defines the forward pass of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2, bidirectional=False):\n",
    "        \"\"\"\n",
    "        Initializes the LSTMRegressor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features per frame.\n",
    "        hidden_size : int, optional\n",
    "            Number of features in the hidden state (default=128).\n",
    "        num_layers : int, optional\n",
    "            Number of recurrent layers (default=2).\n",
    "        dropout : float, optional\n",
    "            Dropout probability between LSTM layers (default=0.2).\n",
    "        bidirectional : bool, optional\n",
    "            If True, use a bidirectional LSTM (default=False).\n",
    "        \"\"\"\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        \n",
    "        direction = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * direction, 2)  # Predict pitch and yaw\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTMRegressor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor\n",
    "            Output tensor of shape (batch_size, 2) containing predicted pitch and yaw.\n",
    "        \"\"\"\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
    "                        x.size(0),\n",
    "                        self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
    "                        x.size(0),\n",
    "                        self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size * num_directions)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size * num_directions)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)  # (batch_size, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_pipeline.py (continued)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25, device='cpu'): # device='cuda'\n",
    "    \"\"\"\n",
    "    Trains the LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The LSTMRegressor model.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for validation data.\n",
    "    criterion : nn.Module\n",
    "        Loss function.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for model parameters.\n",
    "    num_epochs : int, optional\n",
    "        Number of training epochs (default=25).\n",
    "    device : str, optional\n",
    "        Device to train on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The trained model.\n",
    "    history : dict\n",
    "        Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = None\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = val_loader\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in tqdm(dataloader, desc=f'{phase.capitalize()}'):\n",
    "                inputs = inputs.to(device)  # Shape: (batch_size, seq_length, input_size)\n",
    "                # Skip batches where labels are None (i.e., for videos 5-9)\n",
    "                if labels is None:\n",
    "                    continue\n",
    "                \n",
    "                labels = labels.to(device)  # Shape: (batch_size, 2)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  # Shape: (batch_size, 2)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass and optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / len([data for data in dataloader.dataset if data[1] is not None])  # Count only labeled data\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            \n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f}')\n",
    "            \n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(f'Best Validation Loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device='cpu'): # device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained LSTMRegressor model.\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for test data.\n",
    "    criterion : nn.Module\n",
    "        Loss function.\n",
    "    device : str, optional\n",
    "        Device to evaluate on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    test_loss : float\n",
    "        Mean loss on the test set.\n",
    "    predictions : list of np.ndarray\n",
    "        List containing predicted pitch and yaw angles.\n",
    "    targets : list of np.ndarray\n",
    "        List containing actual pitch and yaw angles.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            targets.append(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    \n",
    "    return test_loss, predictions, targets\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss over epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : dict\n",
    "        Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained model.\n",
    "    path : str\n",
    "        Path to save the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "def load_model(model, path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the model weights from the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model architecture to load weights into.\n",
    "    path : str\n",
    "        Path to the saved model weights.\n",
    "    device : str, optional\n",
    "        Device to load the model on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The model with loaded weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples in training dataset: 1\n",
      "Total samples in validation dataset: 0\n",
      "Sample 0:\n",
      "  Features shape: torch.Size([10, 16, 2048, 7, 7])\n",
      "  Labels: None\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 44\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Check a sample of the dataset\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):  \u001b[38;5;66;03m# Checking first 3 samples as an example\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Career/comma/openenv/lib/python3.8/site-packages/torch/utils/data/dataset.py:391\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# lstm_pipeline.py (continued)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate the LSTM model for pitch and yaw prediction.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    FEATURE_DIR = 'data'  # Directory containing feature .pt files\n",
    "    LABEL_DIR = 'labeled'                # Directory containing label .txt files\n",
    "    SEQUENCE_LENGTH = 10 #10                 # Number of frames per sequence\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-3\n",
    "    HIDDEN_SIZE = 128\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "    BIDIRECTIONAL = False\n",
    "    MODEL_SAVE_PATH = 'lstm_regressor.pth'\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    RANDOM_SEED = 42\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # Initialize Dataset\n",
    "    dataset = VideoDataset(data_dir=FEATURE_DIR,\n",
    "                           labeled_dir=LABEL_DIR,\n",
    "                           sequence_length=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * VALIDATION_SPLIT)\n",
    "    train_size = total_size - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size],\n",
    "                                                               generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "\n",
    "    print(f\"Total samples in training dataset: {len(train_dataset)}\")\n",
    "    print(f\"Total samples in validation dataset: {len(val_dataset)}\")\n",
    "\n",
    "    # Check a sample of the dataset\n",
    "    for i in range(3):  # Checking first 3 samples as an example\n",
    "        inputs, labels = train_dataset[i]\n",
    "        print(f\"Sample {i}:\")\n",
    "        print(f\"  Features shape: {inputs.shape}\")\n",
    "        print(f\"  Labels: {labels}\")\n",
    "        \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize the LSTM model\n",
    "    # Assuming each feature vector is flattened (C*H*W)\n",
    "    sample_feature, _ = dataset[0]\n",
    "    input_size = sample_feature.shape[1]  # C*H*W\n",
    "    model = LSTMRegressor(input_size=input_size,\n",
    "                          hidden_size=HIDDEN_SIZE,\n",
    "                          num_layers=NUM_LAYERS,\n",
    "                          dropout=DROPOUT,\n",
    "                          bidirectional=BIDIRECTIONAL)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                                        num_epochs=NUM_EPOCHS, device=DEVICE)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(trained_model, MODEL_SAVE_PATH)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_predictions, val_targets = evaluate_model(trained_model, val_loader, criterion, device=DEVICE)\n",
    "    print(f\"Validation MSE Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save predictions and targets for further evaluation\n",
    "    np.savetxt('val_predictions.txt', val_predictions, fmt='%.6f')\n",
    "    np.savetxt('val_targets.txt', val_targets, fmt='%.6f')\n",
    "    \n",
    "    # Optionally, visualize some predictions vs actual\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot Pitch\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(val_targets[:, 0], label='Actual Pitch', color='blue')\n",
    "    plt.plot(val_predictions[:, 0], label='Predicted Pitch', color='green', linestyle='--')\n",
    "    plt.title('Actual vs Predicted Pitch')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Pitch (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Yaw\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(val_targets[:, 1], label='Actual Yaw', color='blue')\n",
    "    plt.plot(val_predictions[:, 1], label='Predicted Yaw', color='green', linestyle='--')\n",
    "    plt.title('Actual vs Predicted Yaw')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Yaw (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
