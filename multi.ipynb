{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Approach\n",
    "\n",
    "The following notebook will attempt to use CNN feature extraction and LSTM for temporal prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process imports\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Features saved to data/video_9_features.pt\n",
      "Feature extraction completed for video extracted_frames/video_9\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CNNFeatureExtractor:\n",
    "    \"\"\"\n",
    "    A class to extract features from video frames using a pretrained CNN (ResNet).\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The pretrained CNN model (ResNet).\n",
    "    transform : torchvision.transforms.Compose\n",
    "        The transformations applied to input frames (resizing, normalization).\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    extract_features(frame_batch: torch.Tensor) -> torch.Tensor:\n",
    "        Extracts features from a batch of frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    #def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    def __init__(self, device='cpu'):\n",
    "        \"\"\"\n",
    "        Initialize the CNN feature extractor with a pretrained ResNet model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        device : str\n",
    "            The device on which to run the model ('cuda' or 'cpu').\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        # Remove the classification head (fc layer)\n",
    "        self.model = nn.Sequential(*list(self.model.children())[:-2])\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Define the necessary image transformations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),  # Resize frame to 224x224 (ResNet input size)\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def extract_features(self, frame_batch):\n",
    "        \"\"\"\n",
    "        Extract features from a batch of frames.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame_batch : torch.Tensor\n",
    "            A batch of video frames (B, C, H, W).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        features : torch.Tensor\n",
    "            Extracted CNN features for each frame in the batch.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            frame_batch = frame_batch.to(self.device)\n",
    "            features = self.model(frame_batch)\n",
    "        return features\n",
    "\n",
    "    def process_frame(self, frame_path):\n",
    "        \"\"\"\n",
    "        Process a single frame from an image file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame_path : str\n",
    "            Path to the image file (frame).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        processed_frame : torch.Tensor\n",
    "            Processed frame ready for feature extraction.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load image and apply transformations\n",
    "            frame = Image.open(frame_path).convert(\"RGB\")\n",
    "            processed_frame = self.transform(frame)\n",
    "            return processed_frame\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing frame {frame_path}: {str(e)}\")\n",
    "\n",
    "    def process_batch(self, frame_paths):\n",
    "        \"\"\"\n",
    "        Process a batch of frames from a list of image paths.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        frame_paths : list of str\n",
    "            List of file paths to the frames.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        frame_batch : torch.Tensor\n",
    "            A batch of processed frames ready for feature extraction.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            processed_frames = [self.process_frame(fp) for fp in frame_paths]\n",
    "            frame_batch = torch.stack(processed_frames)\n",
    "            return frame_batch\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing frame batch: {str(e)}\")\n",
    "\n",
    "    def save_features(self, features, output_dir, video_id):\n",
    "        \"\"\"\n",
    "        Save extracted features to a file.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        features : torch.Tensor\n",
    "            Extracted features from the CNN.\n",
    "        output_dir : str\n",
    "            Directory to save the features.\n",
    "        video_id : str\n",
    "            Identifier for the video (used in the output filename).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, f\"{video_id}_features.pt\")\n",
    "            torch.save(features.cpu(), output_path)\n",
    "            print(f\"Features saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error saving features: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def extract_video_features(video_frame_dir, output_dir, extractor):\n",
    "    \"\"\"\n",
    "    Extract features for all frames in a video directory and save to output directory.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    video_frame_dir : str\n",
    "        Path to the directory containing video frames.\n",
    "    output_dir : str\n",
    "        Path to the directory where features will be saved.\n",
    "    extractor : CNNFeatureExtractor\n",
    "        The CNN feature extractor instance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all frame files in the directory\n",
    "        frame_files = sorted([os.path.join(video_frame_dir, f) for f in os.listdir(video_frame_dir)\n",
    "                              if f.endswith(('.jpg'))])\n",
    "\n",
    "        # Process frames in batches (if needed for larger videos)\n",
    "        batch_size = 16\n",
    "        for i in range(0, len(frame_files), batch_size):\n",
    "            batch_files = frame_files[i:i + batch_size]\n",
    "            frame_batch = extractor.process_batch(batch_files)\n",
    "            features = extractor.extract_features(frame_batch)\n",
    "            extractor.save_features(features, output_dir, video_frame_dir.split('/')[-1])\n",
    "            \n",
    "        print(f\"Feature extraction completed for video {video_frame_dir}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error extracting features for video {video_frame_dir}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Initialize the feature extractor\n",
    "extractor = CNNFeatureExtractor()\n",
    "\n",
    "# Example: Process video frames from a directory and save extracted features\n",
    "video_frame_directory = 'extracted_frames/video_0'\n",
    "#video_frame_directory = 'extracted_frames/video_9'\n",
    "output_directory = 'data'\n",
    "extract_video_features(video_frame_directory, output_directory, extractor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Dataset\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for loading video features and corresponding pitch and yaw labels.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    feature_dir : str\n",
    "        Directory containing CNN-extracted feature .pt files.\n",
    "    label_dir : str\n",
    "        Directory containing pitch and yaw label .txt files.\n",
    "    sequence_length : int\n",
    "        Number of consecutive frames to include in each sequence.\n",
    "    transform : callable, optional\n",
    "        Optional transform to be applied on a sample.\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    __len__():\n",
    "        Returns the total number of sequences across all videos.\n",
    "    __getitem__(idx):\n",
    "        Retrieves a sequence of features and corresponding labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dir, label_dir, sequence_length=30, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the VideoDataset.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        feature_dir : str\n",
    "            Path to the directory with CNN-extracted feature .pt files.\n",
    "        label_dir : str\n",
    "            Path to the directory with label .txt files.\n",
    "        sequence_length : int\n",
    "            Length of each sequence for LSTM input.\n",
    "        transform : callable, optional\n",
    "            Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.feature_dir = feature_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        \n",
    "        # List all feature and label files\n",
    "        self.video_ids = sorted([f.replace('_features.pt', '').split('_')[1] for f in os.listdir(feature_dir) if f.endswith('_features.pt')])\n",
    "        \n",
    "        # Verify corresponding label files exist\n",
    "        for vid in self.video_ids:\n",
    "            label_path = os.path.join(label_dir, f\"{vid}.txt\") # f\"{vid}_labels.txt\"\n",
    "            if vid == '5':\n",
    "                break\n",
    "\n",
    "            if not os.path.isfile(label_path):\n",
    "                raise FileNotFoundError(f\"Label file for video '{vid}' not found at '{label_path}'\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of sequences across all videos.\n",
    "        \"\"\"\n",
    "        total_sequences = 0\n",
    "        for vid in self.video_ids:\n",
    "            features = torch.load(os.path.join(self.feature_dir, f\"{vid}_features.pt\"))  # Shape: (C, H, W, F)\n",
    "            num_frames = features.shape[-1]\n",
    "            if num_frames >= self.sequence_length:\n",
    "                total_sequences += num_frames - self.sequence_length + 1\n",
    "        return total_sequences\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves a sequence of features and corresponding labels based on the index.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        idx : int\n",
    "            Index of the sequence to retrieve.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        sequence_features : torch.Tensor\n",
    "            Tensor of shape (sequence_length, feature_dim).\n",
    "        sequence_labels : torch.Tensor\n",
    "            Tensor of shape (sequence_length, 2) containing pitch and yaw.\n",
    "        \"\"\"\n",
    "        # Iterate through videos to find which video the idx falls into\n",
    "        cumulative = 0\n",
    "        for vid in self.video_ids:\n",
    "            features = torch.load(os.path.join(self.feature_dir, f\"{vid}_features.pt\"))  # Shape: (C, H, W, F)\n",
    "            num_frames = features.shape[-1]\n",
    "            if num_frames < self.sequence_length:\n",
    "                continue\n",
    "            if idx < cumulative + (num_frames - self.sequence_length + 1):\n",
    "                sequence_idx = idx - cumulative\n",
    "                # Extract feature sequence\n",
    "                feature_sequence = features[:, :, :, sequence_idx : sequence_idx + self.sequence_length]  # Shape: (C, H, W, S)\n",
    "                # Flatten the spatial dimensions (C, H, W) into a single feature vector per frame\n",
    "                feature_sequence = feature_sequence.permute(3, 0, 1, 2)  # Shape: (S, C, H, W)\n",
    "                feature_sequence = feature_sequence.view(self.sequence_length, -1)  # Shape: (S, C*H*W)\n",
    "                \n",
    "                # Load corresponding labels\n",
    "                label_path = os.path.join(self.label_dir, f\"{vid}.txt\") #f\"{vid}_labels.txt\")\n",
    "                labels = np.loadtxt(label_path)  # Shape: (F, 2)\n",
    "                label_sequence = labels[sequence_idx : sequence_idx + self.sequence_length, :]  # Shape: (S, 2)\n",
    "                label_sequence = torch.from_numpy(label_sequence).float()\n",
    "                \n",
    "                if self.transform:\n",
    "                    feature_sequence = self.transform(feature_sequence)\n",
    "                \n",
    "                return feature_sequence, label_sequence\n",
    "            cumulative += (num_frames - self.sequence_length + 1)\n",
    "        \n",
    "        raise IndexError(f\"Index {idx} out of range for dataset with length {len(self)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM-based Regressor for predicting pitch and yaw angles from CNN-extracted features.\n",
    "    \n",
    "    Attributes:\n",
    "    -----------\n",
    "    input_size : int\n",
    "        Size of the input feature vector per frame.\n",
    "    hidden_size : int\n",
    "        Number of features in the hidden state of the LSTM.\n",
    "    num_layers : int\n",
    "        Number of recurrent layers in the LSTM.\n",
    "    dropout : float\n",
    "        Dropout probability between LSTM layers.\n",
    "    bidirectional : bool\n",
    "        If True, becomes a bidirectional LSTM.\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    forward(x):\n",
    "        Defines the forward pass of the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, dropout=0.2, bidirectional=False):\n",
    "        \"\"\"\n",
    "        Initializes the LSTMRegressor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_size : int\n",
    "            Number of input features per frame.\n",
    "        hidden_size : int, optional\n",
    "            Number of features in the hidden state (default=128).\n",
    "        num_layers : int, optional\n",
    "            Number of recurrent layers (default=2).\n",
    "        dropout : float, optional\n",
    "            Dropout probability between LSTM layers (default=0.2).\n",
    "        bidirectional : bool, optional\n",
    "            If True, use a bidirectional LSTM (default=False).\n",
    "        \"\"\"\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidirectional)\n",
    "        \n",
    "        direction = 2 if bidirectional else 1\n",
    "        self.fc = nn.Linear(hidden_size * direction, 2)  # Predict pitch and yaw\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the LSTMRegressor.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape (batch_size, sequence_length, input_size).\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out : torch.Tensor\n",
    "            Output tensor of shape (batch_size, 2) containing predicted pitch and yaw.\n",
    "        \"\"\"\n",
    "        # Initialize hidden and cell states with zeros\n",
    "        h0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
    "                        x.size(0),\n",
    "                        self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * (2 if self.bidirectional else 1),\n",
    "                        x.size(0),\n",
    "                        self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_size * num_directions)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = out[:, -1, :]  # (batch_size, hidden_size * num_directions)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        out = self.fc(out)  # (batch_size, 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_pipeline.py (continued)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=25, device='cpu'): # device='cuda'\n",
    "    \"\"\"\n",
    "    Trains the LSTM model.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The LSTMRegressor model.\n",
    "    train_loader : DataLoader\n",
    "        DataLoader for training data.\n",
    "    val_loader : DataLoader\n",
    "        DataLoader for validation data.\n",
    "    criterion : nn.Module\n",
    "        Loss function.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Optimizer for model parameters.\n",
    "    num_epochs : int, optional\n",
    "        Number of training epochs (default=25).\n",
    "    device : str, optional\n",
    "        Device to train on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The trained model.\n",
    "    history : dict\n",
    "        Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_wts = None\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                dataloader = train_loader\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                dataloader = val_loader\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            \n",
    "            # Iterate over data\n",
    "            for inputs, labels in tqdm(dataloader, desc=f'{phase.capitalize()}'):\n",
    "                inputs = inputs.to(device)  # Shape: (batch_size, seq_length, input_size)\n",
    "                labels = labels.to(device)  # Shape: (batch_size, 2)\n",
    "                \n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  # Shape: (batch_size, 2)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # Backward pass and optimize only in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            \n",
    "            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f}')\n",
    "            \n",
    "            # Deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_val_loss:\n",
    "                best_val_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(f'Best Validation Loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device='cpu'): # device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on the test set.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained LSTMRegressor model.\n",
    "    test_loader : DataLoader\n",
    "        DataLoader for test data.\n",
    "    criterion : nn.Module\n",
    "        Loss function.\n",
    "    device : str, optional\n",
    "        Device to evaluate on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    test_loss : float\n",
    "        Mean loss on the test set.\n",
    "    predictions : list of np.ndarray\n",
    "        List containing predicted pitch and yaw angles.\n",
    "    targets : list of np.ndarray\n",
    "        List containing actual pitch and yaw angles.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing'):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            targets.append(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader.dataset)\n",
    "    predictions = np.vstack(predictions)\n",
    "    targets = np.vstack(targets)\n",
    "    \n",
    "    return test_loss, predictions, targets\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss over epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : dict\n",
    "        Dictionary containing training and validation loss history.\n",
    "    \"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n",
    "    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The trained model.\n",
    "    path : str\n",
    "        Path to save the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error saving model: {str(e)}\")\n",
    "\n",
    "def load_model(model, path, device='cuda'):\n",
    "    \"\"\"\n",
    "    Loads the model weights from the specified path.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : nn.Module\n",
    "        The model architecture to load weights into.\n",
    "    path : str\n",
    "        Path to the saved model weights.\n",
    "    device : str, optional\n",
    "        Device to load the model on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : nn.Module\n",
    "        The model with loaded weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(path, map_location=device))\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/0_features.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 102\u001b[0m\n\u001b[1;32m     99\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 33\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m VideoDataset(feature_dir\u001b[38;5;241m=\u001b[39mFEATURE_DIR,\n\u001b[1;32m     29\u001b[0m                        label_dir\u001b[38;5;241m=\u001b[39mLABEL_DIR,\n\u001b[1;32m     30\u001b[0m                        sequence_length\u001b[38;5;241m=\u001b[39mSEQUENCE_LENGTH)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Split into training and validation sets\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m total_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m val_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(total_size \u001b[38;5;241m*\u001b[39m VALIDATION_SPLIT)\n\u001b[1;32m     35\u001b[0m train_size \u001b[38;5;241m=\u001b[39m total_size \u001b[38;5;241m-\u001b[39m val_size\n",
      "Cell \u001b[0;32mIn[28], line 64\u001b[0m, in \u001b[0;36mVideoDataset.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m total_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m vid \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_ids:\n\u001b[0;32m---> 64\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_features.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Shape: (C, H, W, F)\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_frames \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msequence_length:\n",
      "File \u001b[0;32m~/Documents/Career/comma/openenv/lib/python3.8/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Documents/Career/comma/openenv/lib/python3.8/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Documents/Career/comma/openenv/lib/python3.8/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/0_features.pt'"
     ]
    }
   ],
   "source": [
    "# lstm_pipeline.py (continued)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to train and evaluate the LSTM model for pitch and yaw prediction.\n",
    "    \"\"\"\n",
    "    # Configuration\n",
    "    FEATURE_DIR = 'data'  # Directory containing feature .pt files\n",
    "    LABEL_DIR = 'labeled'                # Directory containing label .txt files\n",
    "    SEQUENCE_LENGTH = 30                 # Number of frames per sequence\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 1e-3\n",
    "    HIDDEN_SIZE = 128\n",
    "    NUM_LAYERS = 2\n",
    "    DROPOUT = 0.2\n",
    "    BIDIRECTIONAL = False\n",
    "    MODEL_SAVE_PATH = 'lstm_regressor.pth'\n",
    "    VALIDATION_SPLIT = 0.2\n",
    "    RANDOM_SEED = 42\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    \n",
    "    # Initialize Dataset\n",
    "    dataset = VideoDataset(feature_dir=FEATURE_DIR,\n",
    "                           label_dir=LABEL_DIR,\n",
    "                           sequence_length=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * VALIDATION_SPLIT)\n",
    "    train_size = total_size - val_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size],\n",
    "                                                               generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Initialize the LSTM model\n",
    "    # Assuming each feature vector is flattened (C*H*W)\n",
    "    sample_feature, _ = dataset[0]\n",
    "    input_size = sample_feature.shape[1]  # C*H*W\n",
    "    model = LSTMRegressor(input_size=input_size,\n",
    "                          hidden_size=HIDDEN_SIZE,\n",
    "                          num_layers=NUM_LAYERS,\n",
    "                          dropout=DROPOUT,\n",
    "                          bidirectional=BIDIRECTIONAL)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model, history = train_model(model, train_loader, val_loader, criterion, optimizer,\n",
    "                                        num_epochs=NUM_EPOCHS, device=DEVICE)\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "    \n",
    "    # Save the trained model\n",
    "    save_model(trained_model, MODEL_SAVE_PATH)\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    val_loss, val_predictions, val_targets = evaluate_model(trained_model, val_loader, criterion, device=DEVICE)\n",
    "    print(f\"Validation MSE Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save predictions and targets for further evaluation\n",
    "    np.savetxt('val_predictions.txt', val_predictions, fmt='%.6f')\n",
    "    np.savetxt('val_targets.txt', val_targets, fmt='%.6f')\n",
    "    \n",
    "    # Optionally, visualize some predictions vs actual\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot Pitch\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(val_targets[:, 0], label='Actual Pitch', color='blue')\n",
    "    plt.plot(val_predictions[:, 0], label='Predicted Pitch', color='green', linestyle='--')\n",
    "    plt.title('Actual vs Predicted Pitch')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Pitch (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot Yaw\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(val_targets[:, 1], label='Actual Yaw', color='blue')\n",
    "    plt.plot(val_predictions[:, 1], label='Predicted Yaw', color='green', linestyle='--')\n",
    "    plt.title('Actual vs Predicted Yaw')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('Yaw (degrees)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
